# -*- coding: utf-8 -*-
"""pali_sat_sft.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vC5FGBkIicx2Gf1kQtWNF33Z3524Ykvp
"""

import os
import gc
import re
import json
import torch
import argparse
import transformers
from datetime import datetime
from dataclasses import dataclass, field
from typing import Optional, Union, Any, List, Dict
from collections import defaultdict
from packaging import version
from PIL import Image
from transformers import (
    AutoProcessor,
    PaliGemmaForConditionalGeneration,
    GenerationConfig,
    PreTrainedModel,
    PreTrainedTokenizerBase,
    Trainer,
    TrainerCallback,
    is_wandb_available,
    Qwen2VLForConditionalGeneration,
)
from transformers.integrations.deepspeed import is_deepspeed_zero3_enabled
from transformers.utils import is_peft_available
from datasets import Dataset, IterableDataset
from torch.utils.data import SequentialSampler
from trl import GRPOConfig, ModelConfig, get_peft_config
from trl.data_utils import apply_chat_template, is_conversational, maybe_apply_chat_template
from trl.models import create_reference_model, prepare_deepspeed, unwrap_model_for_generation

if is_peft_available():
    from peft import PeftConfig, get_peft_model

if is_wandb_available():
    import wandb


RewardFunc = Union[str, PreTrainedModel, callable]

from huggingface_hub import login
login(token='')

dataset_prefix = "../SAT/"
# dataset_path = "SAT_train_filtered_sorted.json"

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

def accuracy_reward(completions, solution, **kwargs):
    if isinstance(completions[0], str):
        contents = [completion for completion in completions]
    else:
        contents = [completion[0]["content"] for completion in completions]
    rewards = []
    correct_count = 0
    total_count = 0


    for content, sol in zip(contents, solution):
        total_count += 1
        # print(content, sol)

        if content.strip() == "":
            reward = -0.2
        else:
            reward = 0.0
            content = re.sub(r"[\(\)\.\,\-]", "", content)
            sol = re.sub(r"[\(\)\.\,\-]", "", sol)

            if content.lower() == sol.lower():
                reward = 1.0
                correct_count += 1

        rewards.append(reward)

    return rewards

class PaliGemmaGRPOTrainer(Trainer):
    def __init__(
        self,
        model: Union[str, PreTrainedModel],
        reward_funcs: Union[RewardFunc, List[RewardFunc]],
        args: GRPOConfig = None,
        train_dataset: Optional[Union[Dataset, IterableDataset]] = None,
        eval_dataset: Optional[Union[Dataset, IterableDataset, Dict[str, Union[Dataset, IterableDataset]]]] = None,
        processing_class: Optional[PreTrainedTokenizerBase] = None,
        reward_processing_classes: Optional[Union[PreTrainedTokenizerBase, List[PreTrainedTokenizerBase]]] = None,
        callbacks: Optional[List[TrainerCallback]] = None,
        optimizers: tuple[Optional[torch.optim.Optimizer], Optional[torch.optim.lr_scheduler.LambdaLR]] = (None, None),
        peft_config: Optional["PeftConfig"] = None,
        max_pixels: Optional[int] = 12845056,
        min_pixels: Optional[int] = 3136,
        attn_implementation: str = "flash_attention_2",
        pad_token_id: Optional[int] = None,
    ):
        # Args
        if args is None:
            model_name = model if isinstance(model, str) else model.config._name_or_path
            model_name = model_name.split("/")[-1]
            args = GRPOConfig(f"{model_name}-GRPO")

        # Models
        # Trained model
        model_init_kwargs = args.model_init_kwargs or {}
        model_init_kwargs["attn_implementation"] = attn_implementation
        model_init_kwargs["torch_dtype"] = torch.bfloat16
        if isinstance(model, str):
            model_id = model

            if "paligemma" in model_id.lower():
                # PaLI-Gemma do not accept use_cache
                if "use_cache" in model_init_kwargs:
                    model_init_kwargs.pop("use_cache")
                model = PaliGemmaForConditionalGeneration.from_pretrained(model, **model_init_kwargs)
            elif "qwen" in model_id.lower():
                model_init_kwargs["use_cache"] = (
                    False if args.gradient_checkpointing else model_init_kwargs.get("use_cache")
                )
                if "Qwen2-VL" in model_id:
                    model = Qwen2VLForConditionalGeneration.from_pretrained(model, **model_init_kwargs)

        else:
            model_id = model.config._name_or_path
            if args.model_init_kwargs is not None:
                raise ValueError(
                    "You passed `model_init_kwargs` to the `GRPOConfig`, but your model is already instantiated. "
                    "This argument can only be used when the `model` argument is a string."
                )

        # PEFT
        if peft_config is not None:
            model = get_peft_model(model, peft_config)

        if processing_class is None:
            if "Qwen2-VL" in model_id:
                processing_class = AutoProcessor.from_pretrained(model_id)
                pad_token_id = processing_class.tokenizer.pad_token_id
                processing_class.pad_token_id = pad_token_id
                processing_class.eos_token_id = processing_class.tokenizer.eos_token_id
                processing_class.image_processor.max_pixels = max_pixels
                processing_class.image_processor.min_pixels = min_pixels
            elif "paligemma" in model_id:
                processing_class = AutoProcessor.from_pretrained(model_id)
                pad_token_id = processing_class.tokenizer.pad_token_id
                processing_class.pad_token_id = pad_token_id
                processing_class.eos_token_id = processing_class.tokenizer.eos_token_id

        def data_collator(features):
            return features

        # Training arguments
        self.max_prompt_length = args.max_prompt_length
        self.max_completion_length = args.max_completion_length


        model.warnings_issued["estimate_tokens"] = True

        self._metrics = defaultdict(list)

        super().__init__(
            model=model,
            args=args,
            data_collator=data_collator,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            processing_class=processing_class,
            callbacks=callbacks,
            optimizers=optimizers,
        )
        # Gradient accumulation requires scaled loss. Normally, loss scaling in the parent class depends on whether the
        # model accepts loss-related kwargs. Since we compute our own loss, this check is irrelevant. We set
        # self.model_accepts_loss_kwargs to False to enable scaling.
        self.model_accepts_loss_kwargs = False

    def _set_signature_columns_if_needed(self):
        if self._signature_columns is None:
            self._signature_columns = ["prompt"]


    def _prepare_inputs(self, inputs: Dict[str, Union[torch.Tensor, Any]]) -> Dict[str, Union[torch.Tensor, Any]]:
        return inputs


    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        prompts = [x["prompt"] for x in inputs]

        prompt_texts = []
        for example in inputs:
            if isinstance(example["prompt"], list) and isinstance(example["prompt"][0], dict) and "content" in example["prompt"][0]:
                content_list = example["prompt"][0]["content"]
                text = ""
                for item in content_list:
                    if item["type"] == "text":
                        text = item["text"]
                        break
                prompt_texts.append(text)
            else:
                prompt_texts.append(str(example["prompt"]))
        images = []
        for x in inputs:
            img_temp = Image.open(dataset_prefix + x["image_path"]).convert("RGB")
            images.append(img_temp)

        prompt_inputs = self.processing_class(
            text=prompt_texts,
            images=images,
            return_tensors="pt",
            padding=True,
            padding_side="left",
            add_special_tokens=True,
        )

        if hasattr(self, "accelerator") and self.accelerator is not None:
            prompt_inputs = {k: v.to(self.accelerator.device) for k, v in prompt_inputs.items()}

        target_texts = [x["solution"] for x in inputs]

        batch_size = prompt_inputs["input_ids"].size(0)
        max_length = prompt_inputs["input_ids"].size(1)

        labels = torch.full((batch_size, max_length), -100, dtype=torch.long)

        if hasattr(self, "accelerator") and self.accelerator is not None:
            labels = labels.to(self.accelerator.device)

        for i, target in enumerate(target_texts):
            target_ids = self.processing_class.tokenizer.encode(
                target, add_special_tokens=False, return_tensors="pt"
            ).squeeze()

            if hasattr(self, "accelerator") and self.accelerator is not None:
                target_ids = target_ids.to(self.accelerator.device)

            if target_ids.dim() == 0:
                target_ids = target_ids.unsqueeze(0)

            target_length = min(len(target_ids), 8)
            preserve_length = max_length - target_length

            if preserve_length < max_length:
                prompt_inputs["input_ids"][i, preserve_length:preserve_length+target_length] = target_ids[:target_length]
                labels[i, preserve_length:preserve_length+target_length] = target_ids[:target_length]

        prompt_inputs["labels"] = labels

        # print(f"Input shape: {prompt_inputs['input_ids'].shape}")
        # print(f"Labels shape: {prompt_inputs['labels'].shape}")

        forward_kwargs = {
            k: v for k, v in prompt_inputs.items()
            if k in ["input_ids", "attention_mask", "pixel_values", "labels", "image_grid_thw"]
        }

        outputs = model(**forward_kwargs)
        loss = outputs.loss

        if hasattr(self, "_metrics"):
            if "loss" not in self._metrics:
                self._metrics["loss"] = []
            self._metrics["loss"].append(loss.item())

        return loss

    def log(self, logs: Dict[str, float], start_time: Optional[float] = None) -> None:
        metrics = {key: sum(val) / len(val) for key, val in self._metrics.items()}
        logs = {**logs, **metrics}
        if version.parse(transformers.__version__) >= version.parse("4.47.0.dev0"):
            super().log(logs, start_time)
        else:
            super().log(logs)
        self._metrics.clear()

def make_conversation_sat(example, base_model_prompt=False):
    QUESTION_TEMPLATE = "{Question}"
    return {
        "image_path": example["images"][0],
        "prompt": [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": QUESTION_TEMPLATE.format(Question=example["messages"][0]["content"])},
                ],
            },
        ],
        "solution": example["messages"][1]["content"],
    }

def train_paligemma_sat(
    model_name="google/paligemma2-3b-mix-224",
    dataset_prefix = "/content/drive/MyDrive/VLLM/SAT/",
    dataset_path = "SAT_train_15000.json",
    output_dir="/content/drive/MyDrive/VLLM/paligemma2-sat-grpo",
    num_train_epochs=1,
    learning_rate=1e-4,
    per_device_train_batch_size=8,
    gradient_accumulation_steps=2,
    max_steps=None,
    logging_steps=10,
    save_steps=1000,
    max_prompt_length=256,
    max_completion_length=128,
    num_generations=4,
    beta=0.01,
    train_samples=100,
    use_wandb=True,
    attn_implementation="flash_attention_2",
    push_to_hub=False,
    seed=42,
    resume_from_checkpoint=None):



    os.environ["DEBUG_MODE"] = "true"
    os.environ["LOG_PATH"] = "grpo_paligemma_sat_training.log"
    torch.manual_seed(seed)


    model_args = ModelConfig(
        model_name_or_path=model_name,
        attn_implementation=attn_implementation,
        use_peft=True,
        lora_r=16,
        lora_alpha=32,
        lora_dropout=0.05,
        # bias="none",
        lora_target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        # lora_target_modules=["q_proj", "v_proj"],

    )


    training_args = GRPOConfig(
        output_dir=output_dir,
        num_train_epochs=num_train_epochs,
        per_device_train_batch_size=per_device_train_batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,
        learning_rate=learning_rate,
        weight_decay=0.01,
        max_grad_norm=1.0,
        max_steps=max_steps,
        logging_steps=logging_steps,
        save_steps=save_steps,
        save_total_limit=3,
        bf16=torch.cuda.is_available(),
        seed=seed,
        data_seed=seed,
        max_prompt_length=max_prompt_length,
        max_completion_length=max_completion_length,
        num_generations=num_generations,
        beta=beta,
        report_to="wandb" if use_wandb else "none",
        run_name=f"paligemma-sat-grpo-{datetime.now().strftime('%Y%m%d-%H%M')}",
        gradient_checkpointing=False,
        push_to_hub=push_to_hub
    )

    print("load data...")
    with open(dataset_prefix + dataset_path, 'r') as f:
      sat_dataset = json.load(f)

    if train_samples > 0 and train_samples < len(sat_dataset):
        sat_dataset = sat_dataset[:train_samples]

    processed_data = [make_conversation_sat(sample) for sample in sat_dataset]

    reward_funcs = [accuracy_reward]


    print("initialize trainer...")
    trainer = PaliGemmaGRPOTrainer(
        model=model_args.model_name_or_path,
        reward_funcs=reward_funcs,
        args=training_args,
        train_dataset=processed_data,
        peft_config=get_peft_config(model_args),
        attn_implementation=attn_implementation,
        max_pixels=401408,
        min_pixels=3136,
    )


    original_torch_load = torch.load
    torch.load = lambda *args, **kwargs: original_torch_load(*args, **{**kwargs, 'weights_only': False})

    # if use_wandb:
    #     wandb.init(
    #         project="huggingface",
    #         name="keeping Paligemma-SAT",
    #         id="jdtc9bjp",
    #         resume="must"
    #     )

    print("Start training...")
    try:
        print("load checkpoints from:",resume_from_checkpoint)
        trainer.train(resume_from_checkpoint=resume_from_checkpoint)
    finally:
        torch.load = original_torch_load

    print("finish training!")


    print("保存模型...")
    trainer.save_model(output_dir)

    # if push_to_hub:
    #     trainer.push_to_hub()

    return trainer

def main():
    parser = argparse.ArgumentParser(description="使用GRPO训练PaLI-Gemma模型处理SAT数据集")
    parser.add_argument("--model_name", type=str, default="google/paligemma2-3b-mix-448", help="模型名称或路径")
    parser.add_argument("--dataset_path", type=str, required=True, help="数据集路径")
    parser.add_argument("--output_dir", type=str, default="./paligemma2-sat-grpo", help="输出目录")
    parser.add_argument("--num_train_epochs", type=int, default=1, help="训练轮数")
    parser.add_argument("--learning_rate", type=float, default=2e-5, help="学习率")
    parser.add_argument("--per_device_train_batch_size", type=int, default=2, help="每个设备的批量大小")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=16, help="梯度累积步数")
    parser.add_argument("--max_steps", type=int, default=None, help="最大训练步数，覆盖epoch设置")
    parser.add_argument("--logging_steps", type=int, default=10, help="日志记录步数")
    parser.add_argument("--save_steps", type=int, default=2, help="保存模型的步数")
    parser.add_argument("--max_prompt_length", type=int, default=256, help="最大提示长度")
    parser.add_argument("--max_completion_length", type=int, default=128, help="最大生成长度")
    parser.add_argument("--num_generations", type=int, default=4, help="GRPO生成数量")
    parser.add_argument("--beta", type=float, default=0.01, help="GRPO的beta参数")
    parser.add_argument("--train_samples", type=int, default=2, help="使用的训练样本数量")
    parser.add_argument("--use_wandb", action="store_true", help="是否使用Weights & Biases记录训练过程")
    parser.add_argument("--attn_implementation", type=str, default="flash_attention_2",
                        help="注意力实现方式，如'flash_attention_2'或'eager'")
    parser.add_argument("--push_to_hub", action="store_true", help="是否推送到Hugging Face Hub")
    parser.add_argument("--seed", type=int, default=42, help="随机种子")

    args = parser.parse_args()

    train_paligemma_sat(
        model_name=args.model_name,
        dataset_path=args.dataset_path,
        output_dir=args.output_dir,
        num_train_epochs=args.num_train_epochs,
        learning_rate=args.learning_rate,
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        max_steps=args.max_steps,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        max_prompt_length=args.max_prompt_length,
        max_completion_length=args.max_completion_length,
        num_generations=args.num_generations,
        beta=args.beta,
        # train_samples=args.train_samples,
        use_wandb=args.use_wandb,
        attn_implementation=args.attn_implementation,
        push_to_hub=args.push_to_hub,
        seed=args.seed,
    )

def run_training():

    dataset_prefix = "../SAT/"
    dataset_path = "SAT_train_15000.json"


    trainer = train_paligemma_sat(
        model_name="google/paligemma2-3b-mix-224",
        # model_name='Qwen/Qwen2-VL-2B-Instruct',
        dataset_prefix = dataset_prefix,
        dataset_path = dataset_path,
        output_dir="./paligemma2-sat-sft",
        # output_dir="./qwen2-sat-grpo",
        num_train_epochs=2,
        learning_rate=1e-5,
        per_device_train_batch_size=8,
        gradient_accumulation_steps=2,
        max_steps=2000,
        logging_steps=10,
        save_steps=50,
        max_prompt_length=2000,
        max_completion_length=128,
        num_generations=1,
        train_samples=15000,
        use_wandb=True,
        attn_implementation="flash_attention_2",
        push_to_hub=False,
        resume_from_checkpoint=None,
        seed=42,
    )

    return trainer

from huggingface_hub import login
login(token='')
run_training()



